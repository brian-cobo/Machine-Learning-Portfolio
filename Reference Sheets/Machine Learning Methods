Linear Regression
    Goal: 
        Find the slope and intercept pair that minimizes loss on average    across all data
    Examples:
        Predicting cost of a house given market price vs. square footage
        Predicting Tax rate of a contry v its GDP
    Equation: y = mx + b
    Calculate Error: Sum of (y-ypredicted)^2
    To Use: 
        from sklearn.linear_model import LinearRegression
        variable = LinearRegression()
        variable.fit(X, y)
        predicted = variable.predict(X)

Multiple Linear Regression
    Goal: 
        Predict the values of a dependent variable using two or more        independent variables.
    Examples:
        Predict the value of a house given square feet, rooms, etc...
    Equation: 
        y = b + m1x1 + m2x2 + ... + mnxn
    Calculate Error: (Residual Analysis, difference between acutal y and                        predicted y) Produces R^2 which tells you how much                      variation there is in the y variable. The closer to                     1 the better.
                        LinearRegression.score(x_train, y_train)
                        LinearRegression.score(x_test, y_test)
    To Use:
        Split and test data first
        from sklearn.linear_model import LinearRegression
        variable = LinearRegression()
        variable.fit(x_train, y_train)
        predicted = variable.predict(x_test)
    Side Notes:
        Can print out coefficient of slope, and intercept. Can be used to see which columns affect the predict value most.
        print(variable.coef_)
        print(variable.intercept_)
        Get accuracy of model
        variable.score(x_test, y_test)

K Nearest Neighbors
    Goal:
        Normalize the data
        find k nearest Neighbors
        classify the new point based on those neighbors
    Examples:
        Predict whether an email is spam or not
        Predict whether it will rain or not
        Predict whether a user is a power user or a casual user
    To Use:
        from sklearn.neighbors import KNeighborsClassifier
        classifier = KNeighborsClassifier(n_neighbors = 3)
        training_points = [
        [0.5, 0.2, 0.1],
        [0.9, 0.7, 0.3],
        [0.4, 0.5, 0.7]
        ]

        training_labels = [0, 1, 1]
        classifier.fit(training_points, training_labels)

        unknown_points = [
        [0.2, 0.1, 0.7],
        [0.4, 0.7, 0.6],
        [0.5, 0.8, 0.1]
        ]

        guesses = classifier.predict(unknown_points)

Naive Bayes Theorem
    Goal: The probability theory that relates conditional probabilities. If A and B denote two events, P(A. | B) denotes the conditional probability of A occurring, given that B occurs.
    Examples:
        Breaking German Enigma
        Calculating probability of an event, given a probability of another
    Formula: P(A|B) = P(B|A) * P(A) / P(B)
    To Use:
        from sklearn.feature_extraction.text import CountVectorizer
        from sklearn.naive_bayes import MultinomialNB

        vectorizer = CountVectorizer()
        vectorizer.fit(["Training review one", "Second review"])
        counts = vectorizer.transform(["one review two review"])
        print(vectorizer.vocabulary_)
        print(review_counts.toarray())
        classifier = MultinomialNB()
        classifier.fit(training_counts, training_labels)
        print(classifier.predict(review_counts))
        print(classifier.predict_proba(review_counts))

Logistic Regression
    Goal: 
        A mathematical model used in statistics to estimate (guess) the probability of an event occurring having been given some previous data
    Examples: 
        -Disease survival —Will a patient, 5 years after treatment for a disease, still be alive?
        -Customer conversion —Will a customer arriving on a sign-up page enroll in a service?
    Formula: z=b0 + b1*x1 +⋯+ bn*xn
    To Use: 
        from sklearn.linear_model import LinearRegression
        model = LinearRegression()
        model.fit(hours_studied,passed_exam)
        probability = model.predict(sample_x).ravel()

Decision Trees 
    Goal:
    Examples:
    Formula:
    To Use:
        from sklearn.tree import DecisionTreeClassifier
        classifier = DecisionTreeClassifier()
        classifier.fit(training_points, training_labels)
        print(classifier.score(testing_points, testing_labels))

        from sklearn.ensemble import RandomForestClassifier

        classifier = RandomForestClassifier(n_estimators = 2000, random_state = 0)
        classifier.fit(training_points, training_labels)
        print(classifier.score(testing_points, testing_labels))
        

Splitting testing and training data
    from sklearn.model_selection import train_test_split
    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2)

def sigmoid(z):
    denominator = 1 + np.exp(-z)
    return 1/denominator


Vocab to know:
    Regression:
        Regression is used to predict outputs that are continuous. The outputs are quantities that can be flexibly determined based on the inputs of the model rather than being confined to a set of possible labels.
    
    Classification:
        Classification is used to predict a discrete label. The outputs fall under a finite set of possible outcomes. Many situations have only two possible outcomes. This is called binary classification (True/False, 0 or 1)

    Gradient Descent: 
        Moving towards the point of the slope that decreases                    loss the most
    Convergence: 
        When the loss stops changing, or slows down tremendously
    
    Normalization Techniques
        (value-minimum)/(maximum-minimum)
        Min-max normalization: 
            Guarantees all features will have the exact same scale but does not handle outliers well.
        Z-score normalization: 
            Handles outliers, but does not produce normalized data with the exact same scale.
    Independent Variable:
        If two events are independent, then the occurrence of one event does not affect the probability of the other event
    Dependent Variable:
        If two events are dependent, then when one event occurs, the probability of the other event occurring changes in a predictable way.
    Conditional Probability:
        The probability that two events happen. Easiest to calulate when two events are independent